"""
Script: Heston Model Training (Deep Differential Network)
=======================================================

Purpose:
    Trains the Deep Differential Network (DDN) to approximate the Heston option 
    pricing formula and its partial derivatives (Greeks).

    Unlike standard regression training, this script handles the complexities of 
    "Differential Machine Learning," specifically the scaling logic required 
    when training on both values and gradients simultaneously.

Key Operations:
    1.  Data Loading: Loads the synthetic dataset (parameters + prices + gradients)
        generated by 'generate_dataset.py'.
    2.  Dual Scaling: 
        -   Features (Inputs) are scaled to [-1, 1] to aid convergence of Tanh/Softplus.
        -   Labels (Prices) are scaled to [0, 1].
    3.  Gradient Adjustment (Chain Rule):
        -   The network computes d(Scaled_Price) / d(Scaled_Input).
        -   The dataset contains d(Raw_Price) / d(Raw_Input).
        -   We mathematically transform the dataset target gradients to match the 
            scaled space of the network: Target = Raw_Grad * (Scale_Y / Scale_X).
    4.  Model Initialization: 
        -   Loads hyperparameter specs from 'models/best_hps.json' (if available).
        -   Otherwise, defaults to the paper's architecture (6 layers, 150 neurons).
    5.  High-Precision Optimization:
        -   Uses AdamW (Adam + Decoupled Weight Decay) to prevent overfitting without
            the noise of Dropout.
        -   Implements 'CosineDecayRestarts' learning rate schedule to escape local 
            minima and settle into a high-precision global minimum (Loss ~ 1e-6).

Outputs:
    -   'models/ddn.weights.h5': The best trained model weights.
    -   'models/scaler_x.save': Input scaler (required for inference).
    -   'models/scaler_y.save': Output scaler (required for inference).
"""

import numpy as np                                      # For numerical operations
import tensorflow as tf                                 # For Deep Learning
import json                                             # For hyperparameter loading
from sklearn.model_selection import train_test_split    # For train/test split
from sklearn.preprocessing import MinMaxScaler          # For scaling
import joblib                                           # For saving scalers
import sys                                              # For file paths
from pathlib import Path                                # For file paths

# Add project root to path to import the Deep Differential Network
project_root = Path(__file__).resolve().parents[1]
sys.path.append(str(project_root))

from model.ddn import DeepDifferentialNetwork            # For the Deep Differential Network architecture
import config                                            # For configuration

def main() -> None:
    """
    Trains the Deep Differential Network (DDN) to approximate the Heston option
    pricing formula and its partial derivatives (Greeks).

    This script performs the following steps:

    1. Data Loading: Loads the synthetic dataset (parameters + prices + gradients)
        generated by 'generate_dataset.py'.
    2. Dual Scaling: 
        -   Features (Inputs) are scaled to [-1, 1] to aid convergence of Tanh/Softplus.
        -   Labels (Prices) are scaled to [0, 1].
    3. Gradient Adjustment (Chain Rule):
        -   The network computes d(Scaled_Price) / d(Scaled_Input).
        -   The dataset contains d(Raw_Price) / d(Raw_Input).
        -   We mathematically transform the dataset target gradients to match the
            scaled space of the network: Target = Raw_Grad * (Scale_Y / Scale_X).
    4. Model Initialization: 
        -   Loads hyperparameter specs from 'models/best_hps.json' (if available).
        -   Otherwise, defaults to the paper's architecture (6 layers, 150 neurons).
    5. High-Precision Optimization:
        -   Uses AdamW (Adam + Decoupled Weight Decay) to prevent overfitting without
            the noise of Dropout.
        -   Implements 'CosineDecayRestarts' learning rate schedule to escape local
            minima and settle into a high-precision global minimum (Loss ~ 1e-6).

    Outputs:
        -   'models/ddn.weights.h5': The best trained model weights.
        -   'models/scaler_x.save': Input scaler (required for inference).
        -   'models/scaler_y.save': Output scaler (required for inference).
    """
    print("Setting seed")
    config.set_reproducibility() 
    
    print("Loading data...")
    data = np.load(config.HESTON_DATASET_PATH)
    X = data['features']
    Y = data['labels']
    
    # --- 1. Scaling ---
    # Inputs to [-1, 1] helps the tanh/softplus activations center around 0
    scaler_x = MinMaxScaler(feature_range=(-1, 1))
    X_s = scaler_x.fit_transform(X)
    
    # Outputs to [0, 1] (Prices are positive)
    scaler_y = MinMaxScaler(feature_range=(0, 1))
    Y_s = scaler_y.fit_transform(Y)
    
    # --- 2. Gradient Adjustment ---
    # We must adjust the target gradients because we are training on Scaled Data.
    # This needs to be done since we stretched/shrunk both the x-axis and y-axis during preprocessing -> the slope (gradient) changed.
    # Chain Rule: d(Scaled_Y)/d(Scaled_X) = dY/dX * (Scale_Y / Scale_X)
    s_x_params = scaler_x.scale_[:5]
    s_y_price = scaler_y.scale_[0]
    
    target_price = Y[:, 0:1] * s_y_price + scaler_y.min_[0]
    target_grads = Y[:, 1:] * (s_y_price / s_x_params)
    Y_final = np.hstack([target_price, target_grads])
    
    # Split into training and validation set
    x_tr, x_val, y_tr, y_val = train_test_split(X_s, Y_final, test_size=config.TRAIN_VALIDATION_SET_SIZE, random_state=42)
    
    # Save Scalers for Backtesting
    joblib.dump(scaler_x, config.SCALER_X_PATH)
    joblib.dump(scaler_y, config.SCALER_Y_PATH)
    
    # --- 3. Model Architecture ---
    if config.BEST_HPS_FILE.exists():
        print(f"Loading tuned architecture from {config.BEST_HPS_FILE}...")
        with open(config.BEST_HPS_FILE, 'r') as f:
            hp = json.load(f)
        
        model = DeepDifferentialNetwork(
            num_hidden=hp['num_hidden'],
            neurons=hp['neurons'],
            dropout=hp['dropout'],
            activation=hp['activation']
        )
        initial_lr = hp['learning_rate']
        decay_epochs = hp['first_decay_epochs']
    else:
        print("Using Default Paper Architecture (6x150).")
        # Paper Specs: 6 Layers, 150 Neurons, Softplus
        model = DeepDifferentialNetwork(num_hidden=6, neurons=150, dropout=0.0, activation='softplus')
        initial_lr = 0.001
        decay_epochs = 100

    # --- 4. High-Precision Optimization ---
    # We use AdamW (Weight Decay) + Cosine Decay for maximum stability.
    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(
        initial_learning_rate=initial_lr,
        first_decay_steps=decay_epochs * (len(x_tr) // config.BATCH_SIZE),
        t_mul=2.0,
        m_mul=0.9,
        alpha=1e-6
    )
    
    optimizer = tf.keras.optimizers.AdamW(
        learning_rate=lr_schedule, 
        weight_decay=1e-6
    )
   
    early_stop = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=config.EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=1
    )
    
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        config.WEIGHTS_PATH, save_best_only=True, save_weights_only=True, monitor='val_loss'
    )
    
    model.compile(optimizer=optimizer)
    
    print("Starting training...")
    history = model.fit(
        x_tr, y_tr,
        validation_data=(x_val, y_val),
        epochs=config.EPOCHS,
        batch_size=config.BATCH_SIZE,
        callbacks=[early_stop, checkpoint],
        verbose=1
    )
    
    best_val = min(history.history['val_loss'])
    print(f"Training Complete. Best Validation Loss: {best_val:.8f}")

if __name__ == "__main__":
    main()