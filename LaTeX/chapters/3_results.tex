\subsection{Hyperparameter Optimization Results}
To identify the optimal configuration for the \ac{ddn}, a hyperparameter search was conducted using the Hyperband algorithm as described in the methodology. The search aimed to minimize the validation loss on a held-out portion of the synthetic dataset. The resulting optimal set of hyperparameters, which was subsequently used for the final model training and backtesting, is summarized in Table \ref{tab:optimal_hyperparameters}.

This configuration achieved a final validation loss ($L_{\text{total}}$) of approximately $2.09 \times 10^{-5}$ during the tuning process which is comparable to the validation loss found by \textcite{zhang2025calibratinghestonmodeldeep} who reported $3.26 \times 10^{-5}$. A notable outcome of the optimization is the selection of a zero dropout rate, indicating that for this specific architecture and dataset, the combination of the AdamW optimizer's weight decay and the complexity of the differential learning task provided sufficient regularization against overfitting.

\begin{table}[H]
    \centering
    \caption{Optimal Hyperparameters Determined by the Hyperband Algorithm.}
    \label{tab:optimal_hyperparameters}
    \begin{threeparttable}
    \begin{tabular}{ll}
        \toprule
        Hyperparameter & Optimal Value \\
        \midrule
        Number of Hidden Layers & 7 \\
        Neurons per Hidden Layer & 64 \\
        Dropout Rate & 0.0 \\
        Activation Function & swish \\
        Initial Learning Rate & 0.0005 \\
        First Decay Epochs & 50 \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
\end{table}

\subsection{Backtesting Performance and Robustness Analysis}
The primary objective of the historical backtest is to evaluate the robustness and accuracy of the \ac{ddn} calibration methodology across a wide range of market conditions. This subsection analyzes the daily calibration performance from 2016 to 2023, with a particular focus on periods of market stress.

The time-series evolution of the daily calibration error is presented in Figure \ref{fig:robustness_analysis}. This figure plots both the in-sample \ac{mre}, calculated on the 80\% of options used for calibration, and the out-of-sample \ac{mre}, calculated on the 20\% held-out set. The 20-day realized volatility of the underlying \ac{aapl} stock is overlaid to provide market context.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../plots/plot_1_stress_test.png}
    \caption{Time-series of daily in-sample and out-of-sample \ac{mre} from the Heston model calibration, plotted against the 20-day realized volatility of \ac{aapl} stock. Key market stress events are annotated with vertical dotted lines.}
    \label{fig:robustness_analysis}
\end{figure}

Several key observations can be drawn from this analysis. First, the out-of-sample \ac{mre} consistently tracks the in-sample \ac{mre} very closely throughout the entire seven-year period. This indicates a very small generalization gap, suggesting that the model is not overfitting to the specific subset of options used for calibration each day and successfully captures the underlying volatility surface. Second, the calibration error is strongly positively correlated with the realized market volatility. During periods of low, stable volatility, the \ac{mre} consistently remains below 5\%. Conversely, spikes in calibration error coincide directly with spikes in realized volatility. This is particularly evident during the annotated stress periods, such as the "Volmageddon" event in early 2018 and the "COVID Crash" in March 2020. During the peak of the COVID-19 crisis, the out-of-sample \ac{mre} reached its maximum observed level of approximately 16\%. However, it is critical to note that the error remained bounded and reverted to its baseline level as market volatility subsided, demonstrating the robustness of the calibration procedure.

To quantify these observations, the backtest period was segmented into five distinct macroeconomic and market regimes. The aggregated performance metrics for each regime are presented in Table \ref{tab:regime_performance}. The analysis confirms that the highest average out-of-sample \ac{mre} occurred during the "COVID Crash" regime, at 6.76\%. This period also exhibited the highest standard deviation of error (2.46\%), indicating more erratic day-to-day calibration performance, which is consistent with the extreme market turbulence at the time. In contrast, the model achieved its highest accuracy during the "Inflation" regime of 2022-2023, with an average out-of-sample \ac{mre} of just 4.09\%.

\begin{table}[H]
    \centering
    \caption{Calibration Performance and Parameter Stability Across Market Regimes.}
    \label{tab:regime_performance}
    \begin{threeparttable}
    \begin{tabular}{lrrrrrrr}
        \toprule
        Regime & Avg \ac{mre} & Std \ac{mre} & Std $\kappa$ & Std $\lambda$ & Std $\sigma$ & Std $\rho$ & Std $v_0$ \\
        \midrule
        \makecell[l]{Pre-Volmageddon \\ \textit{\footnotesize (Jan 2015 - Jan 2018)}} & 0.058 & 0.021 & 0.942 & 0.016 & 0.172 & 0.171 & 0.030 \\
        \makecell[l]{Trade War \\ \textit{\footnotesize (Feb 2018 - Jan 2020)}} & 0.059 & 0.022 & 0.889 & 0.028 & 0.182 & 0.174 & 0.038 \\
        \makecell[l]{COVID Crash \\ \textit{\footnotesize (Feb 2020 - May 2020)}} & 0.068 & 0.025 & 0.851 & 0.031 & 0.217 & 0.247 & 0.160 \\
        \makecell[l]{Recovery \\ \textit{\footnotesize (Jun 2020 - Dec 2021)}} & 0.056 & 0.021 & 0.792 & 0.028 & 0.328 & 0.182 & 0.073 \\
        \makecell[l]{Inflation \\ \textit{\footnotesize (Jan 2022 - Dec 2023)}} & 0.041 & 0.014 & 0.698 & 0.020 & 0.156 & 0.146 & 0.039 \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \item Note: All values are rounded to three decimal places.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

Furthermore, Table \ref{tab:regime_performance} provides insight into the stability of the calibrated Heston parameters. The standard deviation of the initial variance, $v_0$, was an order of magnitude higher during the "COVID Crash" (0.160) than in calmer periods like "Pre-Volmageddon" (0.030). This is a financially intuitive result, as $v_0$ represents the current level of market variance, which was exceptionally volatile during the crash. Similarly, the volatility of volatility, $\sigma$, shows the highest instability during the "Recovery" period, possibly reflecting market uncertainty about the path of the economic recovery. The fact that the calibrated parameters adapt in a theoretically consistent manner provides further validation for the calibration methodology.

A direct comparison of the overall backtesting performance with the results reported in the reference study by \textcite{zhang2025calibratinghestonmodeldeep} provides further context for the model's accuracy. Across the entire seven-year backtest period, the calibration methodology achieved an average in-sample \ac{mre} of 5.33\% and an average out-of-sample \ac{mre} of 5.55\%. The reference paper evaluates its \ac{ddn} method on datasets of 10, 50, and 100 Microsoft call options, reporting \ac{mre}s of 0.67\%, 1.86\%, and 4.64\%, respectively, demonstrating a clear degradation in performance as the complexity of the calibration surface increases with the number of contracts. A crucial factor in interpreting these results is the size and diversity of the option set used for daily calibration. In this study, the daily calibration was performed on a significantly larger set of contracts, with an average of 261 options in the in-sample set and 65 options in the out-of-sample set.

To further diagnose the performance characteristics of the daily calibration, Figure \ref{fig:diagnostic_plots} provides a more granular analysis of the model's generalization capability and its pricing accuracy at the individual option level.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../plots/plot_5_regression.png}
    \caption{Diagnostic scatter plots of the backtesting performance. The left panel shows the daily out-of-sample \ac{mre} versus the in-sample \ac{mre}, illustrating the model's generalization gap. The right panel plots the calibrated model price against the observed market price for a large sample of individual options.}
    \label{fig:diagnostic_plots}
\end{figure}

The left panel of Figure \ref{fig:diagnostic_plots} directly assesses the generalization gap by plotting the out-of-sample \ac{mre} against the in-sample \ac{mre} for each day of the backtest. The data points are tightly clustered around the 45-degree line of perfect generalization, which represents the ideal scenario where the model performs identically on the held-out test data as it does on the data used for calibration. The consistently small vertical distance between the observed points and this line indicates that the model does not suffer from significant overfitting. This provides strong evidence that the \ac{ddn}-based calibration is learning a robust representation of the underlying volatility surface each day, rather than simply memorizing the prices of the specific contracts in the calibration set.

The right panel provides a complementary view by examining the pricing accuracy at the level of individual options, aggregated across multiple days in the backtest. The plot shows a very high concentration of points along the identity line, where the model price is equal to the market price. This demonstrates the model's high fidelity in replicating market prices across a wide range of absolute values, from near-zero to over \$350. A degree of heteroscedasticity is observable, where the variance of the absolute pricing error increases for higher-priced, deep-\ac{itm} options. This is an expected statistical artifact in financial modeling and does not detract from the overall conclusion that the calibrated model consistently produces prices that are in close agreement with market observations.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../plots/plot_3_price_fit.png}
    \caption{Calibrated \ac{ddn}-Heston model fit against observed market prices for \ac{aapl} call options on June 15, 2020 (Spot: \$342.99). Each subplot represents a different option maturity, demonstrating the model's fit across the term structure.}
    \label{fig:price_fit_panel}
\end{figure}

To provide a more granular, qualitative assessment of the calibration performance on a single trading day, Figure~\ref{fig:price_fit_panel} presents the model-implied prices against observed market prices for \ac{aapl} options on June 15, 2020. A high degree of correspondence is observed between the calibrated \ac{ddn}-Heston model prices and the market quotes, particularly for short- to medium-term maturities. The model successfully captures the characteristic convex decay of the option price as a function of the strike price across the entire term structure presented. A minor, yet systematic, deviation is observable for the longest-dated options (823 days), where the model appears to slightly underprice options with strikes ranging from \$150 to \$500. This discrepancy may be attributable to several factors, including the reduced liquidity and wider bid-ask spreads typical of long-term options, or the inherent limitations of the Heston model in capturing the term structure of volatility over multi-year horizons. Nevertheless, the visual evidence presented in the figure corroborates the low \ac{mre} metrics reported, demonstrating the model's capability to produce a consistent and accurate fit across a wide range of strikes and maturities for a given day.

Taken together, these diagnostic plots reinforce the findings from the time-series analysis, confirming that the calibration methodology is not only robust across different market conditions but also demonstrates strong generalization and high pricing fidelity at both the aggregate and individual instrument levels.

\subsection{Error Analysis by Moneyness and Maturity}
To disaggregate the overall performance and identify specific areas of strength and weakness, the out-of-sample \ac{mre} was analyzed across different option moneyness and maturity buckets. Figure \ref{fig:mre_heatmaps} presents this analysis for both call options, which are priced directly by the \ac{ddn}, and put options, whose prices are derived using the put-call parity relationship.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../plots/plot_4_heatmap.png}
    \caption{Out-of-sample \ac{mre} stratified by time to maturity and linear moneyness. The left panel shows the \ac{mre} for call options priced directly by the \ac{ddn}. The right panel shows the \ac{mre} for put options, with prices derived from the \ac{ddn}'s call price predictions via put-call parity.}
    \label{fig:mre_heatmaps}
\end{figure}

The analysis of the call option performance reveals a distinct and theoretically consistent pattern. The model achieves its highest accuracy, with \ac{mre}s as low as 1.4\%, for \ac{itm} (moneyness < 0.95) and \ac{atm} (moneyness 0.95-1.05) contracts, particularly those with longer maturities. This region represents the most liquid and highest-priced segment of the call option market. Conversely, the calibration error is highest for \ac{otm} (moneyness > 1.05) calls, reaching a peak of 15.2\% for contracts with less than three months to expiration. This behavior is a known characteristic of the \ac{mre} metric; \ac{otm} options have very low absolute prices, causing even small absolute pricing errors to translate into large relative errors.

The performance for put options, whose prices are derived via parity, shows a notable asymmetry. The model is accurate for \ac{itm} puts (moneyness > 1.05), where the \ac{mre} is consistently low across all maturities, ranging from 1.2\% to 3.5\%. However, the model exhibits significantly higher errors for \ac{otm} puts (moneyness < 0.95), with the \ac{mre} reaching 26.8\% for short- to medium-dated contracts. This phenomenon is a direct consequence of the error propagation through the put-call parity formula. An \ac{otm} put corresponds to an \ac{itm} call. The model's highly accurate pricing of \ac{itm} calls, when used in the parity equation to price the corresponding (and very cheap) \ac{otm} puts, can result in a large relative error for the put. Conversely, the larger relative errors from cheap \ac{otm} calls do not significantly impact the relative error of the corresponding expensive \ac{itm} puts. Overall, the analysis confirms that the \ac{ddn} calibration is most robust in the \ac{atm} region, which is the most critical for practical risk management and volatility trading applications.

\subsection{Stability Across Interest Rate Regimes}
To assess the impact of the prevailing interest rate and dividend yield environment on calibration accuracy, the relationship between the daily out-of-sample \ac{mre} and the dynamically calculated implied risk-free rate ($r-q$) was examined. The backtesting period from 2016 to 2023 encompassed a wide range of monetary conditions, including periods of near-zero and negative implied rates, providing a robust test of the model's stability. Figure \ref{fig:rate_stability} presents a scatter plot of this relationship for every day in the backtest.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{../plots/plot_2_rate_sensitivity.png}
    \caption{Out-of-sample \ac{mre} as a function of the daily implied risk-free rate ($r-q$). Each point represents a single trading day's calibration result. A linear regression line is overlaid to indicate the general trend.}
    \label{fig:rate_stability}
\end{figure}

The analysis reveals a weak, negative linear relationship between the implied risk-free rate and the calibration error. As the implied rate increases, the out-of-sample \ac{mre} exhibits a modest tendency to decrease. Importantly, there is no evidence of performance degradation or instability in the low or negative implied rate regimes. The calibration errors remain well-distributed and bounded across the entire observed spectrum of rates, from approximately -3\% to +4\%.

A plausible interpretation of this negative trend is that periods of very low or negative implied rates often coincide with periods of market stress, where high dividend yields relative to risk-free rates are more common. As established in the time-series analysis, such periods of market stress are typically associated with higher realized volatility, which is the primary driver of increased calibration error. Therefore, the slightly higher \ac{mre} observed at the lower end of the rate spectrum is likely attributable to the confounding effect of volatility rather than the interest rate level itself. The key finding from this analysis is that the \ac{ddn}-based calibration methodology demonstrates considerable robustness and does not exhibit any systematic failure or bias across the diverse interest rate and dividend yield environments encountered during the seven-year backtest. This validates the effectiveness of the dynamic, parity-based rate estimation procedure.